{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import SimpleRNN, Dense, Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential \n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import NaiveBayesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"C:\\\\Users\\\\Archit\\\\Desktop\\\\MachineLearning\\\\ML_projects\\\\twitter\\\\0000000000002747_training_twitter_x_y_train.csv\")\n",
    "test_data = pd.read_csv(\"C:\\\\Users\\\\Archit\\\\Desktop\\\\MachineLearning\\\\ML_projects\\\\twitter\\\\0000000000002747_test_twitter_x_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10980, 12), (3660, 11))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10980,), (10980,), (3660,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['text']\n",
    "y_train = train_data['airline_sentiment']\n",
    "x_test = test_data['text']\n",
    "x_train.shape, y_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['i', 'me', 'my', 'myself', 'we'], 211)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "punct = list(string.punctuation)\n",
    "stop_words = stop+punct\n",
    "stop_words[:5], len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@SouthwestAir I am scheduled for the morning, 2 days after the fact, yes..not sure why my evening flight was the only one Cancelled Flightled',\n",
       "  'negative'),\n",
       " ('@SouthwestAir seeing your workers time in and time out going above and beyond is why I love flying with you guys. Thank you!',\n",
       "  'positive')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating document by appending input and output data.\n",
    "# here we've 3 types of classes i.e negative, positive and neutral.\n",
    "documents = []\n",
    "for i in range(len(x_train)):\n",
    "    documents.append((x_train[i], y_train[i]))\n",
    "documents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_pos_tag_value(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'SouthwestAir',\n",
       " 'I',\n",
       " 'am',\n",
       " 'scheduled',\n",
       " 'for',\n",
       " 'the',\n",
       " 'morning',\n",
       " ',',\n",
       " '2',\n",
       " 'days',\n",
       " 'after',\n",
       " 'the',\n",
       " 'fact',\n",
       " ',',\n",
       " 'yes..not',\n",
       " 'sure',\n",
       " 'why',\n",
       " 'my',\n",
       " 'evening',\n",
       " 'flight',\n",
       " 'was',\n",
       " 'the',\n",
       " 'only',\n",
       " 'one',\n",
       " 'Cancelled',\n",
       " 'Flightled']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(documents[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stop_words addig posTag in documents.\n",
    "def clean_text(text):\n",
    "    clean_text = []\n",
    "    text_lst = word_tokenize(text)\n",
    "    #print(text_lst,end=\" \")\n",
    "    for t in text_lst:\n",
    "        if t.lower() not in stop_words:\n",
    "            postag = pos_tag([t])\n",
    "            #print(type(t))\n",
    "            clean_word = lemma.lemmatize(t, pos = get_pos_tag_value(postag[0][1]))\n",
    "            clean_text.append(clean_word.lower())\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_documents = [(clean_text(text), response) for text, response in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['southwestair', 'schedule', 'morning', '2', 'day', 'fact', 'yes..not', 'sure', 'even', 'flight', 'one', 'cancelled', 'flightled'] negative "
     ]
    }
   ],
   "source": [
    "print(clean_documents[0][0], clean_documents[0][1], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 10980)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(clean_documents), len(clean_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['americanair', 'car', 'gng', 'dfw', 'pulled', '1hr', 'ago', 'icy', 'road', 'on-hold', 'aa', 'since', '1hr', 'ca', \"n't\", 'reach', 'arpt', 'aa2450', 'wat', '2']\n",
      "@AmericanAir In car gng to DFW. Pulled over 1hr ago - very icy roads. On-hold with AA since 1hr. Can't reach arpt for AA2450. Wat 2 do?\n"
     ]
    }
   ],
   "source": [
    "x_test_clean = [clean_text(t) for t in x_test]\n",
    "print(x_test_clean[0])\n",
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['southwestair', 'schedule', 'morning', '2', 'day', 'fact', 'yes..not', 'sure', 'even', 'flight'] "
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for doc in clean_documents:\n",
    "    all_words += doc[0]\n",
    "print(all_words[:10],end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119926"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "freq = nltk.FreqDist(all_words)\n",
    "most_common = freq.most_common(3400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('flight', 3342), ('united', 3178), ('usairways', 2253), ('americanair', 2234), ('southwestair', 1841), ('jetblue', 1762), ('get', 1542), (\"n't\", 1535), (\"'s\", 1113), ('http', 906)] "
     ]
    }
   ],
   "source": [
    "l1 = ['...',  \"''\", '``',  \"''\", '``', ]\n",
    "print(most_common[:10], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3400"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = []\n",
    "for f in most_common:\n",
    "    if f not in l1:\n",
    "        features.append(f[0])\n",
    "# features = features[10:]\n",
    "# features.remove('``')\n",
    "# features.remove(\"''\")\n",
    "# features.remove('â€™')\n",
    "# features.remove('..')\n",
    "\n",
    "# features.remove('...')\n",
    "# features.remove('u')\n",
    "# features.remove('..')\n",
    "# features.remove('..')\n",
    "# features.remove('..')\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flight', 'united', 'usairways', 'americanair', 'southwestair', 'jetblue', 'get', \"n't\", \"'s\", 'http']"
     ]
    }
   ],
   "source": [
    "print(features[:10], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_dic(words, features):\n",
    "    f_dictionary = {}\n",
    "    word_set = set(words)\n",
    "    for f in features:\n",
    "        cnt = 0\n",
    "        for w in word_set:\n",
    "            if f == w:\n",
    "                cnt+=1\n",
    "        f_dictionary[f] = cnt\n",
    "    return f_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [(feature_dic(doc, features), category) for doc, category in clean_documents]\n",
    "#print(training_data[:1],end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'AmericanAir', 'In', 'car', 'gng', 'to', 'DFW', '.', 'Pulled', 'over', '1hr', 'ago', '-', 'very', 'icy', 'roads', '.', 'On-hold', 'with', 'AA', 'since', '1hr', '.', 'Ca', \"n't\", 'reach', 'arpt', 'for', 'AA2450', '.', 'Wat', '2', 'do', '?'] "
     ]
    }
   ],
   "source": [
    "#print(x_test_clean[0],end=\"\")\n",
    "x_test_tokenize = [word_tokenize(x) for x in x_test] \n",
    "print(x_test_tokenize[0], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = [(feature_dic(doc, features)) for doc in x_test_tokenize]\n",
    "#print(testing_data[:2], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NaiveBayesClassifier.train(training_data) \n",
    "# module 'nltk.classify' has no attribute 'predict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8450819672131148"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy by selecting top_most 3400 words\n",
    "nltk.classify.accuracy(clf, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_y_pred = clf.classify_many(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3660"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "southwestair schedule morning 2 day fact yes..not sure even flight one cancelled flightled\n",
      "\n",
      "@ AmericanAir In car gng to DFW . Pulled over 1hr ago - very icy roads . On-hold with AA since 1hr . Ca n't reach arpt for AA2450 . Wat 2 do ?\n"
     ]
    }
   ],
   "source": [
    "all_output_train = [response for text, response in clean_documents]\n",
    "all_input_train = [\" \".join(text) for text, response in clean_documents]\n",
    "#all_input_test =  [\" \".join(text) for text in x_test_clean]\n",
    "all_input_test =  [\" \".join(text) for text in x_test_tokenize]\n",
    "print(all_input_train[0])\n",
    "print()\n",
    "print(all_input_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(max_features = 12500, ngram_range = (1, 2))\n",
    "x_train_new = count_vec.fit_transform(all_input_train)\n",
    "x_test_new = count_vec.transform(all_input_test)\n",
    "y_train_new = all_output_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(x_train_new, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9811475409836066"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(x_train_new, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predrf = rf.predict(x_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=750, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(750, kernel = 'rbf')\n",
    "svc.fit(x_train_new, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8937158469945355"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(x_train_new, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3660"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predsvm = svc.predict(x_test_new)\n",
    "len(y_predsvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy\n",
    "# numpy.savetxt('twitterResult.csv', y_predrf, delimiter = \",\", fmt=\"%s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
